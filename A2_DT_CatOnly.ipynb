{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yUOzXh0KEYf"
   },
   "source": [
    "# Data-Mining Course (EECS 6412)\n",
    "# Assignment (II): Decision Tree Classifier Implementation in Python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "065A5OW9KOFA"
   },
   "source": [
    "\n",
    "## Objective: Implement a Decision Tree classifier in Python to gain a deeper understanding of its working principles.\n",
    "\n",
    "**Overal Instructions:**\n",
    "\n",
    "\n",
    "*   Your task is to implement a Decision Tree classifier in Python.\n",
    "*   The implementation has been broken down into multiple subfunctions, each with accompanying hints. Your goal is to complete the code for each function.\n",
    "* You are only allowed to use the **pandas** and **numpy** libraries for this assignment. Some functions from Pandas have been provided for your convenience in the initial section, and you may use them if you feel they are necessary.\n",
    "* Each part of your solution will be graded separately. However the sections are interrelated. It is crucial that your code is well-documented with comments explaining each part of your implementation.\n",
    "* Please be aware that your responses will be thoroughly reviewed to ensure originality. Plagiarized or copied work will result in penalties.\n",
    "\n",
    "\n",
    "**- Please skip the following descriptions and move directly to the Questions section if you are familiar with reading CSV files with Pandas library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLSAGzhqGnNy"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##Please write your full name/names and student IDs here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Full Name: Parsa Merat\n",
    "*   Student ID: 217554197\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBJOIRZSOiDd"
   },
   "source": [
    "        \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Dataset Description for Car Acceptability Classification:\n",
    " Your codes must be general and should work on each tabular datasets with  categorical data types. For this example, have been provided with two datasets for training and testing- a training dataset (1400 samples) and a test dataset (327 samples). Pleased download datasets from [here](https://drive.google.com/drive/folders/1aka1ySucu1e3PqytQnVdEf63v9LT0E5z?usp=sharing). These samples represent the decisions of car experts regarding the acceptability of cars. The experts have categorized the cars into one of four classes: \"acceptable,\" \"unacceptable,\" \"good,\" or \"very good\" based on six categorical features.\n",
    "\n",
    "# Features:\n",
    "\n",
    "* **'BUYING':** This feature determines the purchase price of the car and is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n",
    "\n",
    "* **'MAINTENANCE':** This feature indicates how high the car's maintenance cost is, and it is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n",
    "\n",
    "* **'DOORS':** This featurte indicates number of the doors each car has: '2', '3', '4', '5more'(5 or more than 5 doors).\n",
    "\n",
    "* **'PERSONS':** This feature determines the car's capacity in terms of the number of persons it can accommodate and is categorized as '2', '4', or 'more'.\n",
    "\n",
    "* **'LUG_BOOT':** This feature represents the size of the car's luggage boot (trunk) and is categorized as 'small', 'med' (medium), or 'big'.\n",
    "\n",
    "* **'SAFETY':** This feature provides an estimate of the car's safety level and is categorized as 'low', 'med' (medium), or 'high'.\n",
    "\n",
    "* **'CLASS':** This is the target variable. It indicates the acceptance level of the car and is categorized as 'unacc' (unacceptable), 'acc' (acceptable), 'good', or 'vgood' (very good).\n",
    "\n",
    "**Please note that in this example the \"CLASS\" attribute is located at the last column of the tabular datasets**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8XuTyhMnwP5"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Accessing the Datasets:\n",
    "To access and read datasets from Google Drive in Google Colab using the Pandas library, you can follow these steps:\n",
    "\n",
    "1.   Upload CSV Files to Google Drive: First, ensure that you've uploaded the CSV files (train dataset and test dataset) to your Google Drive. You can create a folder for your project and upload the files there.\n",
    "\n",
    "\n",
    "2.   Mount Google Drive in Google Colab:mount your Google Drive using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1695758736703,
     "user": {
      "displayName": "hamidreza dastmalchi",
      "userId": "13779278108614878523"
     },
     "user_tz": 240
    },
    "id": "AteP7IIqn4XI",
    "outputId": "57681bf3-03c7-47bb-c178-e0f5994a77cc"
   },
   "outputs": [],
   "source": [
    "# # using jupyter, no need for theese\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7tuWcl8zVkM"
   },
   "source": [
    "\n",
    "\n",
    "3.   Access and Read Data using Pandas: You can access your CSV files in the mounted Google Drive directory. For example, if your CSV files are located in a folder named \"data-mining/assignment2/UG/\" in your Google Drive, you can read them as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w6jZmiO5zwrJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for your CSV files\n",
    "# train_csv_path = '/content/drive/MyDrive/data-mining/assignment2/UG/data_train_c.csv'\n",
    "# test_csv_path = '/content/drive/MyDrive/data-mining/assignment2/UG/data_test_c.csv'\n",
    "train_csv_path = 'data_train_c.csv'\n",
    "test_csv_path = 'data_test_c.csv'\n",
    "\n",
    "# Read the data into Pandas DataFrames\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m34_HjXP2PZB"
   },
   "source": [
    "\n",
    "\n",
    "4.   See Some Samples with head() Function:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695758736703,
     "user": {
      "displayName": "hamidreza dastmalchi",
      "userId": "13779278108614878523"
     },
     "user_tz": 240
    },
    "id": "abW4fCsV2U6O",
    "outputId": "8aae6844-291d-4155-96bd-7ef9db12634e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in the Training Dataset:\n",
      "  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
      "0  vhigh         low      2    more      med   high    acc\n",
      "1  vhigh         low  5more       2      med   high  unacc\n",
      "2    med       vhigh      2       2      med    low  unacc\n",
      "3    med         low      2       2      med    low  unacc\n",
      "4    low         med      2       2      big    low  unacc\n",
      "\n",
      "Samples in the Test Dataset:\n",
      "  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
      "0    med       vhigh      2    more    small   high  unacc\n",
      "1    low         low      4       4    small    med    acc\n",
      "2    low         low      4    more    small    low  unacc\n",
      "3  vhigh       vhigh      4       2      big    med  unacc\n",
      "4  vhigh         med  5more       4    small   high    acc\n"
     ]
    }
   ],
   "source": [
    "# See the first 5 samples in the training dataset\n",
    "print(\"Samples in the Training Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# See the first 5 samples in the test dataset\n",
    "print(\"\\nSamples in the Test Dataset:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xpw1xCD2jM9"
   },
   "source": [
    "\n",
    "\n",
    "5.   Access Feature Names using columns Attribute:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695758736703,
     "user": {
      "displayName": "hamidreza dastmalchi",
      "userId": "13779278108614878523"
     },
     "user_tz": 240
    },
    "id": "LT9x4hgR2sRD",
    "outputId": "2fd00c2b-4106-4ddd-d8bb-0976d30965b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      "Index(['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY',\n",
      "       'CLASS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (column names) of the training dataset\n",
    "feature_names = train_df.columns\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfjUc4iy2wby"
   },
   "source": [
    "6. Access Each Column as a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1695758736703,
     "user": {
      "displayName": "hamidreza dastmalchi",
      "userId": "13779278108614878523"
     },
     "user_tz": 240
    },
    "id": "FGWxsVWr248i",
    "outputId": "f91fd98b-f93d-4783-a8b1-08b06b621677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    vhigh\n",
      "1    vhigh\n",
      "2      med\n",
      "3      med\n",
      "4      low\n",
      "Name: BUYING, dtype: object\n",
      "0      low\n",
      "1      low\n",
      "2    vhigh\n",
      "3      low\n",
      "4      med\n",
      "Name: MAINTENANCE, dtype: object\n",
      "0      acc\n",
      "1    unacc\n",
      "2    unacc\n",
      "3    unacc\n",
      "4    unacc\n",
      "Name: CLASS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Access the 'BUYING' column as a Series using square bracket notation\n",
    "buying_price = train_df['BUYING']\n",
    "print(buying_price.head())\n",
    "\n",
    "# Access the 'MAINTENANCE' column:\n",
    "maintenance_cost = train_df['MAINTENANCE']\n",
    "print(maintenance_cost.head())\n",
    "\n",
    "# Access the 'CLASS' column in the test dataset as a Series\n",
    "labels = train_df['CLASS']\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JfDTOKe3sTm"
   },
   "source": [
    "7. Use value_counts() function to  find the number of samples for each distinct value for a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1695758736703,
     "user": {
      "displayName": "hamidreza dastmalchi",
      "userId": "13779278108614878523"
     },
     "user_tz": 240
    },
    "id": "QKLkuPXm46YG",
    "outputId": "e00d7718-ce23-462a-9bb0-5e2fedf609d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each distinct value in 'BUYING':\n",
      "MAINTENANCE\n",
      "vhigh    355\n",
      "high     355\n",
      "low      350\n",
      "med      340\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts of each distinct value in 'BUYING':\")\n",
    "print (maintenance_cost.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueIDC_tn_lLu"
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Questions\n",
    "---\n",
    "\n",
    "## - Part 1: Check Terminal Node Condition:\n",
    "(Q.1., **5 Marks**): In the first step, we need to check if a node containing a DataFrame is a terminal node or it needs further splitting. Implement a function called \"check_if_terminal\" to do this task.\n",
    "\n",
    "Function Requirements:\n",
    "\n",
    "Input:\n",
    "\n",
    "\n",
    "*   parent_data: the DataFrame corresponding to a node.\n",
    "\n",
    "*   threshold: Proportion threshold for the majority class.\n",
    "\n",
    "\n",
    "\n",
    "Calculate the proportion of samples with the majority class label.\n",
    "\n",
    "If the proportion â‰¥ threshold, return \"Leaf\" as flag.\n",
    "\n",
    "If the proportion < threshold, return \"Internal\" as the flag.\n",
    "\n",
    "In addition to the flag, the function must return majority class (\"acc\"/\"unacc\"/\"good\", \"vgood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dkKqk351BFbr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_if_terminal(dataframe, threshold):\n",
    "    # Get all attribute names from the DataFrame\n",
    "    all_attrs = dataframe.columns\n",
    "    \n",
    "    # Select the last attribute as the class attribute\n",
    "    class_attrs = all_attrs[-1]\n",
    "    \n",
    "    # Extract the labels (values of the class attribute)\n",
    "    labels = dataframe[class_attrs]\n",
    "    \n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    counts = labels.value_counts(sort=False)\n",
    "    majority_class = counts.idxmax() \n",
    "    ratio = counts[majority_class] / len(labels.index)\n",
    "    \n",
    "    flag = \"Internal\" if ratio<threshold else \"Leaf\"\n",
    "    \n",
    "    # output flag must be a string (whether \"Internal\" or \"Leaf\")\n",
    "    # majority_class must be a string indicating the majority label of the samples in the node\n",
    "    #..................................\n",
    "    return flag, majority_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xyHOFah3-1Oh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the node type is Internal\n",
      "the majority class of the node is unacc\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation on training dataframe:\n",
    "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
    "print(\"the node type is {}\".format(flag))\n",
    "print(\"the majority class of the node is {}\".format(majority_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTq3CiKq5SFS"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## - Part 2: Entropy Function:\n",
    "(Q.2.,  **10 Marks**): In order to split a node in a decision tree based on the Information Gain criterion, we need to calculate the entropy of the samples. Entropy is a measure of impurity in the data, and it is used to quantify the uncertainty associated with a set of class labels.\n",
    "\n",
    "\n",
    "**Task:** Write a Python function called \"entropy\" that takes a the CLASS column of the dataframe denoted as \"label\" and returns the entropy as the output.\n",
    "\n",
    "Function Signature: def entropy(labels: list) -> float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MkZoOwiZ7DMz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(labels):\n",
    "    # Count the occurrences of each unique label\n",
    "    value_counts = labels.value_counts()\n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    \n",
    "    probs = value_counts.to_numpy()\n",
    "    probs = probs / len(labels.index)\n",
    "    logs = np.log2(probs)\n",
    "    \n",
    "    entp = np.dot(logs, probs)\n",
    "    \n",
    "    #.................................\n",
    "    #Return the calculated entropy\n",
    "    return -entp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YAj5YHcz_bWG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy of the node is 1.1790359988713874\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation on training dataframe:\n",
    "labels = train_df[\"CLASS\"]\n",
    "entrp = entropy(labels)\n",
    "print(\"entropy of the node is {}\".format(entrp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIUqZqOl8fie"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## - Part 3: Calculating Information Gain:\n",
    "(Q.3., **15 Marks**): In this step, you are required to implement a function named \"information_gain\" that computes the information gain obtained by splitting samples denoted by 'CLASS' column referenced as 'labels' based on a specific attribute column denoted as 'x'. It should be noted that both 'labels' and 'x' are columns of a DataFrame. Please use the function written in \"Part 2\" for this part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "izpdQGkh-PaM"
   },
   "outputs": [],
   "source": [
    "def information_gain(x, labels):\n",
    "    #Calculate the entropy of the parent node\n",
    "    parent_entropy = entropy(labels)\n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    childs_entropy = 0\n",
    "    \n",
    "    for val in x.unique():\n",
    "        devided = labels[x==val]\n",
    "        childs_entropy += len(devided.index) * entropy(devided)\n",
    "    childs_entropy /= len(labels.index)\n",
    "\n",
    "    #Calculate the information gain by subtracting child entropy from parent entropy\n",
    "    info_gain = parent_entropy - childs_entropy\n",
    "    #.................................\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_SbID8sz_uv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information gain of the node in splitting over PERSONS attribute is 0.21326310194104836\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation for training dataframe on \"PERSONS\" attribute:\n",
    "labels = train_df[\"CLASS\"]\n",
    "x = train_df[\"PERSONS\"]\n",
    "info_gain = information_gain(x,labels)\n",
    "print(\"information gain of the node in splitting over PERSONS attribute is {}\".format(info_gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_C2rfRQPNxE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## - Part 4: Selecting the Best Attribute for Splitting\n",
    "(Q.4., **10 Marks**): In this part, you are tasked with implementing a function called \"select_attribute.\" This function will take a parent DataFrame referenced as \"parent_data\" along with a list of splittable attributes denoted by \"remaining_attrs\" as the input and returns a string representing name of the best attribute which yields to the highest information gain after splitting. You may use the function written in \"Part 3\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "id": "PrRSD7ezQyyH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_attribute(parent_data, remaining_attrs):\n",
    "    all_attrs = parent_data.columns\n",
    "    # Extract the class attribute:\n",
    "    class_attr = all_attrs[-1]\n",
    "    \n",
    "    # Extract the labels (target values) from the parent data\n",
    "    labels = parent_data[class_attr]\n",
    "    \n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    # Loop through \"remaining_attrs\" attributes and calculate their information gains\n",
    "    gains = np.zeros(len(remaining_attrs))\n",
    "    for i,attr in enumerate(remaining_attrs):\n",
    "        gains[i] = information_gain(parent_data[attr], labels)\n",
    "    sel_attr = remaining_attrs[gains.argmax()]\n",
    "    \n",
    "    # Find the attribute with the highest information gain and return it as sel_attr\n",
    "    #.................................\n",
    "    \n",
    "    return sel_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8SPm6-CnAgfh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best attribute for splitting the node is SAFETY\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation on training dataframe:\n",
    "remaining_attrs = list(train_df.columns[:-1])\n",
    "sel_attr = select_attribute(train_df, remaining_attrs)\n",
    "print(\"the best attribute for splitting the node is {}\".format(sel_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UH1AkKUrCI8"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "# - Part 5: Splitting the nodes at each tree level\n",
    "\n",
    "(Q.5., **20 Marks**):\n",
    "\n",
    "\n",
    " In this assignment, you will be implementing a crucial part of the decision tree implementation by creating a Python function called data_split. The purpose of this function is to split a parent node's dataframe into child dataframes based on the best attribute, which yields the highest information gain. You may use the helper functions that you have already implemented in previous sections.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "* Write a function called \"data_split\" to split all the nodes in level \"n\" and to generate all the children nodes in level \"n+1\".\n",
    "\n",
    "* Perform node splitting in a systematic manner, progressing level by level. This entails creating all nodes at level n+1 by dividing all nodes eligible for splitting at level n. Refer to the example below for clarification:\n",
    "\n",
    "![Image](https://drive.google.com/uc?export=download&id=1kIOCkYaxUJMEKumBP6RxOLriQQY2Wlqx)\n",
    "  As depicted in the illustration, at level 1, there is a solitary node designated as \"Node_1_1,\" symbolizing the first node of the first level. Level 1 has been subdivided into three nodes, identified as \"Node_2_1,\" \"Node_2_2,\" and \"Node_2_3,\" signifying the first, second, and third nodes of the second level of splitting. Please adhere to this notation for naming each node.\n",
    "\n",
    "* Imagine a dictionary named \"dataframe_dict,\" where the \"keys\" correspond to the node names at a specific splitting level, and the \"values\" represent the associated dataframes. To illustrate, for level 1, the \"dataframe_dict\" would consist of a single key, \"Node_1_1,\" with the corresponding value being the primary dataframe:\n",
    "                dataframe_dict = {\"Node_1_1\": the main dataframe}\n",
    "In this example, following the execution of the \"data_split\" function, the \"dataframe_dict\" dictionary should be replaced with a dictionary containing three entries, as demonstrated below:\n",
    "      dataframe_dict = {\n",
    "                            \"Node_2_1\": dataframe_2_1,\n",
    "                            \"Node_2_2\": dataframe_2_2,\n",
    "                            \"Node_2_3\": dataframe_2_3\n",
    "                        }\n",
    "\n",
    "\n",
    "\n",
    "* Similarly, consider another dictionary called \"remaining_attrs\" with \"keys\" representing the nodes' names, and \"values\" representing the splittable attributes for each node.  For the first level, the \"remaining_attrs\" dictionary might be defined as:\n",
    "\n",
    "      remaining_attrs = {\n",
    "                            \"Node_1_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY']\n",
    "                        }\n",
    "but after running the function \"data_split\", it would be updated to a dictionary with three keys-values as:\n",
    "\n",
    "      remaining_attrs = {\n",
    "                         \"Node_2_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'],\n",
    "                         \"Node_2_2\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'] ,\n",
    "                         \"Node_2_3\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY']\n",
    "                         }\n",
    "\n",
    "Please note that once we've performed a split on a categorical attribute such as \"PERSONS\" and generated the children nodes in the subsequent level, we are no longer permitted to split on the same categorical attribute within that branch of the tree. It's important to emphasize that this restriction doesn't apply to numerical attributes.\n",
    "\n",
    "In the context of this example, this means that the \"remaining_attrs\" dictionary is updated to a three-element dictionary, where none of the nodes in this specific branch have the \"PERSONS\" attribute as a splittable option anymore.\n",
    "\n",
    "* Consider the \"tree_model\" as a list containing three additional dictionaries: \"tree_connectivity\", \"node_labels\", \"and node_types\":\n",
    "\n",
    "            tree_model = [tree_connectivity , node_types, node_labels]\n",
    "\n",
    "where \"tree_connectivity\" is a dictionary representing the node connection to the parents. The \"node_types\" and \"node_labels\" are also dictionaries containing the (\"Leaf\" or \"Internal\") and the majority class for each node, respectively. Your \"data_split\" function must take the \"tree_model\" generated up to  level \"n\" and must update it to the model up top level \"n+1\" after splitting. See the below image for this example:\n",
    "The tree_model at level 1 is:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1GSJzh4CNE298LFXQR86LYpEfj4Q883-S\"  width=400>\n",
    "\n",
    "\n",
    "After running the \"data_split\" function, the tree_model will be updated up to level 2 as follows:\n",
    "\n",
    "![Image](https://drive.google.com/uc?export=download&id=1Y3sGXHBpQtPVpMh8UnVlO0AYXvHNTGfo)\n",
    "\n",
    "\n",
    "**Therefore**: You must write the function \"data_split\" which takes \"dataframe_dict\", \"remaining_attrs\", \"tree_model\", \"level\", and \"threshold\" as the input and update \"dataframe_dict\", \"remaining_attrs\", and \"tree_model\" upto level \"level+1\". The function must also retun a boolean flag \"stop_train\" which must be True if any child node is generated. Otherwise, it must return False. Here, input \"threshold\" is the majority class threshold for checking wether a node is a \"Leaf\" node or an \"Internal\" node.\n",
    "\n",
    "**To complete the function**:\n",
    "* Loop through the nodes in \"dataframe_dict\" in the current \"level\". For each node, check if it's an \"Internal\" node and if so, find the best attribute for splitting. Create child nodes and finally update all the variables.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bAsxLUcTrFtV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold):\n",
    "    # Unpack the tree_model list into three separate variables\n",
    "    [tree_connectivity, node_labels, node_types] = tree_model\n",
    "    \n",
    "    # Create an empty dictionary to store new child dataframes\n",
    "    dataframe_dict_new = {}\n",
    "    remaining_attrs_new = {}\n",
    "    \n",
    "    # Initialize a counter for child nodes\n",
    "    child_ind = 1\n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    # Iterate over keys in the dataframe_dict (representing nodes at the current level)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # I made some small changes tho this part\n",
    "    # dataframe_dict and remaining_attrs are only going to include data for INTERNAL nodes.\n",
    "    # since if a node is not internal and theres no need to split it, why keep its data through levels. (keep in mind Leaves are still saved in tree_modle)\n",
    "    \n",
    "    # i also made tree_conectivity to have different fields for \"the selected attribute\" (example value: DOORS) and \"the attributes value\" (example values: 2, 4, 5more, etc)\n",
    "    # this just made sense to me. rather than having a boolean string (eg: DOORS==2)\n",
    "    \n",
    "    for parent_node, df in dataframe_dict.items():\n",
    "        remaining_list = remaining_attrs[parent_node]\n",
    "        selected_atr = select_attribute(df, remaining_list)\n",
    "\n",
    "        tree_connectivity[parent_node] = {\"split_attribute\":selected_atr, \"attribute_value\":{}} #set the selected attribute in tree_conectivity and create connections to current node's children\n",
    "\n",
    "        remaining_list.remove(selected_atr)\n",
    "        \n",
    "        for val in df[selected_atr].unique(): # for every unique value in the selected_atr, make a node and update the tree model\n",
    "            new_df = df.loc[df[selected_atr]==val]\n",
    "            node_name = \"node_\" + str(level+1) + \"_\" + str(child_ind)\n",
    "            child_ind += 1\n",
    "\n",
    "            flag, majority_class = check_if_terminal(new_df, threshold)\n",
    "            if not remaining_list: #if theres no more remaining attributes to split, set the node to be a leaf\n",
    "                flag = \"Leaf\"\n",
    "                \n",
    "            node_labels[node_name] = majority_class\n",
    "            node_types[node_name] = flag\n",
    "            \n",
    "            tree_connectivity[parent_node][\"attribute_value\"][str(val)] = node_name\n",
    "\n",
    "            if flag == \"Internal\":\n",
    "                dataframe_dict_new[node_name] = new_df\n",
    "                remaining_attrs_new[node_name] = remaining_list.copy()\n",
    "        \n",
    "    \n",
    "    \n",
    "    #replace the new old dictionaries with new dictionaries\n",
    "    dataframe_dict = dataframe_dict_new\n",
    "    remaining_attrs = remaining_attrs_new\n",
    "    stop_train = child_ind==1\n",
    "    # update the tree_model and return it\n",
    "    # also return True as stop_train if no child node is generated. Otherwise return False\n",
    "    #.................................\n",
    "    # Return the updated tree_model and a flag indicating whether training should stop\n",
    "    return tree_model, stop_train, dataframe_dict, remaining_attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TbJDC_sFA5jm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tree connectivity:\n",
      "{'node_1_1': {'split_attribute': 'SAFETY', 'attribute_value': {'high': 'node_2_1', 'low': 'node_2_2', 'med': 'node_2_3'}}}\n",
      "\n",
      " node labels:\n",
      "{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc'}\n",
      "\n",
      " node types:\n",
      "{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal'}\n",
      "\n",
      " remaining attributes are:\n",
      "{'node_2_1': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT'], 'node_2_3': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT']}\n"
     ]
    }
   ],
   "source": [
    "# Now Check your implementation on training dataframe:\n",
    "# Initializing\n",
    "threshold = 0.9\n",
    "\n",
    "tree_connectivity = {}\n",
    "\n",
    "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
    "\n",
    "node_types = {\"node_1_1\": flag}\n",
    "node_labels = {\"node_1_1\": majority_class}\n",
    "\n",
    "# Create an initial tree_model\n",
    "tree_model = [tree_connectivity, node_labels, node_types]\n",
    "\n",
    "# Create an initial dataframe_dict\n",
    "dataframe_dict = {\"node_1_1\": train_df}\n",
    "\n",
    "# Create an initial remaining_attrs\n",
    "\n",
    "independent_attrs = list(train_df.columns[:-1])\n",
    "remaining_attrs = {\"node_1_1\": independent_attrs}\n",
    "\n",
    "# Set level to 1\n",
    "level = 1\n",
    "\n",
    "\n",
    "# Update tree model\n",
    "tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n",
    "\n",
    "\n",
    "[tree_connectivity, node_labels, node_types] = tree_model\n",
    "\n",
    "print(\"\\n tree connectivity:\")\n",
    "print(tree_connectivity)\n",
    "\n",
    "print(\"\\n node labels:\")\n",
    "print(node_labels)\n",
    "\n",
    "print(\"\\n node types:\")\n",
    "print(node_types)\n",
    "\n",
    "print(\"\\n remaining attributes are:\")\n",
    "print(remaining_attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K2ROrMYyUfR"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## -Part 6: Training the Decision Tree\n",
    "\n",
    "(Q.6., **10 Marks**): Now, let's create a function called \"tree_train\" to train the decision tree. This function begins by initializing the tree model and dataframe dictionary using the root node named \"node_1_1.\" It then iteratively updates these structures as it progresses through the tree, continuing until no further child nodes are generated. The process starts at level 1, and with each iteration, the level is incremented. Importantly, make sure to utilize the \"split_data\" function, which you've previously implemented, to assist in the tree construction. Ultimately, the function must return the fully trained tree model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1EVuxiv2zD8c"
   },
   "outputs": [],
   "source": [
    "def tree_train(training_data, threshold):\n",
    "    # Initializing\n",
    "    tree_connectivity = {}\n",
    "    \n",
    "    flag, majority_class = check_if_terminal(training_data, threshold)\n",
    "    \n",
    "    node_types = {\"node_1_1\": flag}\n",
    "    node_labels = {\"node_1_1\": majority_class}\n",
    "    \n",
    "    # Create a tree_model list to store connectivity, node labels, and node types\n",
    "    tree_model = [tree_connectivity, node_labels, node_types]\n",
    "    \n",
    "    # Create a dataframe_dict with the initial training data and associate it with the root node\n",
    "    dataframe_dict = {\"node_1_1\": training_data}\n",
    "    \n",
    "    # Create a remaining_attrs dictionary with all the independent attributes and associate it with the root node\n",
    "    indp_attrs = list(training_data.columns[:-1])\n",
    "    remaining_attrs = {\"node_1_1\": indp_attrs}\n",
    "    \n",
    "    # Initialize the level of the tree to 1\n",
    "    level = 1\n",
    "    \n",
    "    \n",
    "    # Continue tree construction until a stopping condition is met (use while loop)\n",
    "    #.................................\n",
    "    # write the rest here:\n",
    "    # write a loop function and exit the loop if terminating criterion is met\n",
    "    stop_train = flag==\"Leaf\"\n",
    "    while not stop_train:\n",
    "        tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n",
    "        level += 1    \n",
    "    #.................................\n",
    "    # Return the final tree model\n",
    "    return tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cHS5ytxLDEdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tree connectivity:\n",
      "{'node_1_1': {'split_attribute': 'SAFETY', 'attribute_value': {'high': 'node_2_1', 'low': 'node_2_2', 'med': 'node_2_3'}}, 'node_2_1': {'split_attribute': 'PERSONS', 'attribute_value': {'more': 'node_3_1', '2': 'node_3_2', '4': 'node_3_3'}}, 'node_2_3': {'split_attribute': 'PERSONS', 'attribute_value': {'4': 'node_3_4', 'more': 'node_3_5', '2': 'node_3_6'}}, 'node_3_1': {'split_attribute': 'BUYING', 'attribute_value': {'vhigh': 'node_4_1', 'low': 'node_4_2', 'high': 'node_4_3', 'med': 'node_4_4'}}, 'node_3_3': {'split_attribute': 'BUYING', 'attribute_value': {'high': 'node_4_5', 'low': 'node_4_6', 'vhigh': 'node_4_7', 'med': 'node_4_8'}}, 'node_3_4': {'split_attribute': 'BUYING', 'attribute_value': {'vhigh': 'node_4_9', 'low': 'node_4_10', 'high': 'node_4_11', 'med': 'node_4_12'}}, 'node_3_5': {'split_attribute': 'BUYING', 'attribute_value': {'high': 'node_4_13', 'low': 'node_4_14', 'med': 'node_4_15', 'vhigh': 'node_4_16'}}, 'node_4_1': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_1', 'high': 'node_5_2', 'vhigh': 'node_5_3', 'med': 'node_5_4'}}, 'node_4_2': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_5', 'high': 'node_5_6', 'med': 'node_5_7', 'vhigh': 'node_5_8'}}, 'node_4_3': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'vhigh': 'node_5_9', 'high': 'node_5_10', 'low': 'node_5_11', 'med': 'node_5_12'}}, 'node_4_4': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'vhigh': 'node_5_13', 'med': 'node_5_14', 'high': 'node_5_15', 'low': 'node_5_16'}}, 'node_4_5': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_17', 'high': 'node_5_18', 'vhigh': 'node_5_19', 'med': 'node_5_20'}}, 'node_4_6': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'med': 'node_5_21', 'high': 'node_5_22', 'low': 'node_5_23', 'vhigh': 'node_5_24'}}, 'node_4_7': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_25', 'high': 'node_5_26', 'med': 'node_5_27', 'vhigh': 'node_5_28'}}, 'node_4_8': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'high': 'node_5_29', 'med': 'node_5_30', 'low': 'node_5_31', 'vhigh': 'node_5_32'}}, 'node_4_9': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'med': 'node_5_33', 'vhigh': 'node_5_34', 'low': 'node_5_35', 'high': 'node_5_36'}}, 'node_4_10': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'vhigh': 'node_5_37', 'high': 'node_5_38', 'low': 'node_5_39', 'med': 'node_5_40'}}, 'node_4_11': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_5_41', 'med': 'node_5_42', 'big': 'node_5_43'}}, 'node_4_12': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_44', 'vhigh': 'node_5_45', 'med': 'node_5_46', 'high': 'node_5_47'}}, 'node_4_13': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_5_48', 'small': 'node_5_49', 'big': 'node_5_50'}}, 'node_4_14': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'vhigh': 'node_5_51', 'high': 'node_5_52', 'low': 'node_5_53', 'med': 'node_5_54'}}, 'node_4_15': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_55', 'vhigh': 'node_5_56', 'med': 'node_5_57', 'high': 'node_5_58'}}, 'node_4_16': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_5_59', 'vhigh': 'node_5_60', 'high': 'node_5_61', 'med': 'node_5_62'}}, 'node_5_4': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_6_1', '2': 'node_6_2', '4': 'node_6_3', '5more': 'node_6_4'}}, 'node_5_5': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_5', 'med': 'node_6_6', 'small': 'node_6_7'}}, 'node_5_6': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_8', 'big': 'node_6_9', 'med': 'node_6_10'}}, 'node_5_7': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_11', 'big': 'node_6_12', 'med': 'node_6_13'}}, 'node_5_8': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_6_14', '3': 'node_6_15', '2': 'node_6_16', '5more': 'node_6_17'}}, 'node_5_10': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_6_18', '2': 'node_6_19', '5more': 'node_6_20', '3': 'node_6_21'}}, 'node_5_14': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_22', 'med': 'node_6_23', 'big': 'node_6_24'}}, 'node_5_15': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_6_25', '3': 'node_6_26', '4': 'node_6_27', '2': 'node_6_28'}}, 'node_5_16': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_29', 'big': 'node_6_30', 'small': 'node_6_31'}}, 'node_5_21': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_32', 'small': 'node_6_33', 'med': 'node_6_34'}}, 'node_5_22': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_35', 'small': 'node_6_36', 'med': 'node_6_37'}}, 'node_5_23': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_38', 'small': 'node_6_39', 'med': 'node_6_40'}}, 'node_5_30': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_41', 'big': 'node_6_42', 'small': 'node_6_43'}}, 'node_5_31': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_44', 'small': 'node_6_45', 'big': 'node_6_46'}}, 'node_5_33': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_47', 'small': 'node_6_48', 'big': 'node_6_49'}}, 'node_5_35': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_50', 'small': 'node_6_51', 'big': 'node_6_52'}}, 'node_5_37': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_53', 'med': 'node_6_54', 'small': 'node_6_55'}}, 'node_5_39': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_56', 'big': 'node_6_57', 'small': 'node_6_58'}}, 'node_5_40': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_59', 'small': 'node_6_60', 'big': 'node_6_61'}}, 'node_5_42': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_6_62', '3': 'node_6_63', '2': 'node_6_64', '4': 'node_6_65'}}, 'node_5_43': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_6_66', 'vhigh': 'node_6_67', 'med': 'node_6_68', 'high': 'node_6_69'}}, 'node_5_44': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_70', 'med': 'node_6_71', 'small': 'node_6_72'}}, 'node_5_45': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_73', 'small': 'node_6_74', 'big': 'node_6_75'}}, 'node_5_47': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_76', 'big': 'node_6_77', 'med': 'node_6_78'}}, 'node_5_48': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'med': 'node_6_79', 'high': 'node_6_80', 'low': 'node_6_81', 'vhigh': 'node_6_82'}}, 'node_5_50': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'vhigh': 'node_6_83', 'med': 'node_6_84', 'high': 'node_6_85', 'low': 'node_6_86'}}, 'node_5_51': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_6_87', 'big': 'node_6_88', 'small': 'node_6_89'}}, 'node_5_53': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_90', 'small': 'node_6_91', 'med': 'node_6_92'}}, 'node_5_54': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_93', 'med': 'node_6_94', 'small': 'node_6_95'}}, 'node_5_55': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_96', 'med': 'node_6_97', 'small': 'node_6_98'}}, 'node_5_56': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_99', 'med': 'node_6_100', 'small': 'node_6_101'}}, 'node_5_58': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'big': 'node_6_102', 'small': 'node_6_103', 'med': 'node_6_104'}}, 'node_5_59': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_105', 'med': 'node_6_106', 'big': 'node_6_107'}}, 'node_5_62': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_6_108', 'med': 'node_6_109', 'big': 'node_6_110'}}, 'node_6_2': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_7_1', 'small': 'node_7_2', 'big': 'node_7_3'}}, 'node_6_6': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_4', '2': 'node_7_5', '3': 'node_7_6', '5more': 'node_7_7'}}, 'node_6_8': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_8', '4': 'node_7_9', '5more': 'node_7_10', '2': 'node_7_11'}}, 'node_6_11': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_12', '2': 'node_7_13', '4': 'node_7_14'}}, 'node_6_16': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'med': 'node_7_15', 'small': 'node_7_16'}}, 'node_6_19': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_7_17', 'big': 'node_7_18'}}, 'node_6_22': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_7_19', '4': 'node_7_20', '2': 'node_7_21', '3': 'node_7_22'}}, 'node_6_23': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_23', '5more': 'node_7_24', '4': 'node_7_25', '2': 'node_7_26'}}, 'node_6_28': {'split_attribute': 'LUG_BOOT', 'attribute_value': {'small': 'node_7_27', 'med': 'node_7_28'}}, 'node_6_29': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_29', '4': 'node_7_30', '3': 'node_7_31'}}, 'node_6_31': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_32', '4': 'node_7_33', '3': 'node_7_34', '5more': 'node_7_35'}}, 'node_6_34': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_36', '2': 'node_7_37', '5more': 'node_7_38', '3': 'node_7_39'}}, 'node_6_37': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_40', '4': 'node_7_41', '5more': 'node_7_42'}}, 'node_6_40': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_43', '3': 'node_7_44', '2': 'node_7_45'}}, 'node_6_41': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_7_46', '4': 'node_7_47', '2': 'node_7_48', '3': 'node_7_49'}}, 'node_6_44': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_50', '2': 'node_7_51', '5more': 'node_7_52'}}, 'node_6_47': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_53', '4': 'node_7_54', '5more': 'node_7_55'}}, 'node_6_50': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_56', '2': 'node_7_57', '5more': 'node_7_58', '3': 'node_7_59'}}, 'node_6_54': {'split_attribute': 'DOORS', 'attribute_value': {'4': 'node_7_60', '2': 'node_7_61', '5more': 'node_7_62'}}, 'node_6_56': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_63', '4': 'node_7_64', '5more': 'node_7_65', '3': 'node_7_66'}}, 'node_6_59': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_67', '2': 'node_7_68', '5more': 'node_7_69'}}, 'node_6_62': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'med': 'node_7_70', 'low': 'node_7_71', 'vhigh': 'node_7_72', 'high': 'node_7_73'}}, 'node_6_65': {'split_attribute': 'MAINTENANCE', 'attribute_value': {'low': 'node_7_74', 'vhigh': 'node_7_75', 'high': 'node_7_76', 'med': 'node_7_77'}}, 'node_6_71': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_78', '3': 'node_7_79', '5more': 'node_7_80', '4': 'node_7_81'}}, 'node_6_73': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_82', '4': 'node_7_83', '5more': 'node_7_84', '3': 'node_7_85'}}, 'node_6_79': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_86', '3': 'node_7_87', '5more': 'node_7_88', '4': 'node_7_89'}}, 'node_6_80': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_90', '3': 'node_7_91', '5more': 'node_7_92', '4': 'node_7_93'}}, 'node_6_87': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_94', '5more': 'node_7_95', '4': 'node_7_96', '3': 'node_7_97'}}, 'node_6_94': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_7_98', '2': 'node_7_99', '4': 'node_7_100'}}, 'node_6_95': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_101', '5more': 'node_7_102'}}, 'node_6_98': {'split_attribute': 'DOORS', 'attribute_value': {'3': 'node_7_103', '5more': 'node_7_104', '2': 'node_7_105'}}, 'node_6_100': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_106', '3': 'node_7_107', '5more': 'node_7_108', '4': 'node_7_109'}}, 'node_6_104': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_110', '5more': 'node_7_111'}}, 'node_6_106': {'split_attribute': 'DOORS', 'attribute_value': {'2': 'node_7_112', '4': 'node_7_113', '3': 'node_7_114'}}, 'node_6_109': {'split_attribute': 'DOORS', 'attribute_value': {'5more': 'node_7_115', '2': 'node_7_116', '4': 'node_7_117', '3': 'node_7_118'}}}\n",
      "\n",
      " node labels:\n",
      "{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc', 'node_3_1': 'acc', 'node_3_2': 'unacc', 'node_3_3': 'acc', 'node_3_4': 'unacc', 'node_3_5': 'acc', 'node_3_6': 'unacc', 'node_4_1': 'unacc', 'node_4_2': 'vgood', 'node_4_3': 'acc', 'node_4_4': 'acc', 'node_4_5': 'acc', 'node_4_6': 'vgood', 'node_4_7': 'acc', 'node_4_8': 'acc', 'node_4_9': 'unacc', 'node_4_10': 'acc', 'node_4_11': 'unacc', 'node_4_12': 'acc', 'node_4_13': 'unacc', 'node_4_14': 'acc', 'node_4_15': 'acc', 'node_4_16': 'unacc', 'node_5_1': 'acc', 'node_5_2': 'unacc', 'node_5_3': 'unacc', 'node_5_4': 'acc', 'node_5_5': 'vgood', 'node_5_6': 'vgood', 'node_5_7': 'vgood', 'node_5_8': 'acc', 'node_5_9': 'unacc', 'node_5_10': 'acc', 'node_5_11': 'acc', 'node_5_12': 'acc', 'node_5_13': 'acc', 'node_5_14': 'vgood', 'node_5_15': 'acc', 'node_5_16': 'vgood', 'node_5_17': 'acc', 'node_5_18': 'acc', 'node_5_19': 'unacc', 'node_5_20': 'acc', 'node_5_21': 'vgood', 'node_5_22': 'vgood', 'node_5_23': 'good', 'node_5_24': 'acc', 'node_5_25': 'acc', 'node_5_26': 'unacc', 'node_5_27': 'acc', 'node_5_28': 'unacc', 'node_5_29': 'acc', 'node_5_30': 'acc', 'node_5_31': 'vgood', 'node_5_32': 'acc', 'node_5_33': 'unacc', 'node_5_34': 'unacc', 'node_5_35': 'unacc', 'node_5_36': 'unacc', 'node_5_37': 'acc', 'node_5_38': 'acc', 'node_5_39': 'good', 'node_5_40': 'acc', 'node_5_41': 'unacc', 'node_5_42': 'unacc', 'node_5_43': 'acc', 'node_5_44': 'good', 'node_5_45': 'unacc', 'node_5_46': 'acc', 'node_5_47': 'acc', 'node_5_48': 'acc', 'node_5_49': 'unacc', 'node_5_50': 'acc', 'node_5_51': 'acc', 'node_5_52': 'acc', 'node_5_53': 'good', 'node_5_54': 'good', 'node_5_55': 'good', 'node_5_56': 'acc', 'node_5_57': 'acc', 'node_5_58': 'acc', 'node_5_59': 'unacc', 'node_5_60': 'unacc', 'node_5_61': 'unacc', 'node_5_62': 'acc', 'node_6_1': 'acc', 'node_6_2': 'acc', 'node_6_3': 'acc', 'node_6_4': 'acc', 'node_6_5': 'vgood', 'node_6_6': 'vgood', 'node_6_7': 'good', 'node_6_8': 'acc', 'node_6_9': 'vgood', 'node_6_10': 'vgood', 'node_6_11': 'good', 'node_6_12': 'vgood', 'node_6_13': 'vgood', 'node_6_14': 'acc', 'node_6_15': 'acc', 'node_6_16': 'acc', 'node_6_17': 'acc', 'node_6_18': 'acc', 'node_6_19': 'unacc', 'node_6_20': 'acc', 'node_6_21': 'acc', 'node_6_22': 'acc', 'node_6_23': 'vgood', 'node_6_24': 'vgood', 'node_6_25': 'acc', 'node_6_26': 'acc', 'node_6_27': 'acc', 'node_6_28': 'unacc', 'node_6_29': 'vgood', 'node_6_30': 'vgood', 'node_6_31': 'good', 'node_6_32': 'vgood', 'node_6_33': 'good', 'node_6_34': 'vgood', 'node_6_35': 'vgood', 'node_6_36': 'acc', 'node_6_37': 'vgood', 'node_6_38': 'vgood', 'node_6_39': 'good', 'node_6_40': 'good', 'node_6_41': 'vgood', 'node_6_42': 'vgood', 'node_6_43': 'acc', 'node_6_44': 'vgood', 'node_6_45': 'good', 'node_6_46': 'vgood', 'node_6_47': 'acc', 'node_6_48': 'unacc', 'node_6_49': 'acc', 'node_6_50': 'acc', 'node_6_51': 'unacc', 'node_6_52': 'acc', 'node_6_53': 'acc', 'node_6_54': 'acc', 'node_6_55': 'unacc', 'node_6_56': 'acc', 'node_6_57': 'good', 'node_6_58': 'acc', 'node_6_59': 'acc', 'node_6_60': 'acc', 'node_6_61': 'good', 'node_6_62': 'acc', 'node_6_63': 'unacc', 'node_6_64': 'unacc', 'node_6_65': 'acc', 'node_6_66': 'acc', 'node_6_67': 'unacc', 'node_6_68': 'acc', 'node_6_69': 'acc', 'node_6_70': 'good', 'node_6_71': 'acc', 'node_6_72': 'acc', 'node_6_73': 'unacc', 'node_6_74': 'unacc', 'node_6_75': 'acc', 'node_6_76': 'unacc', 'node_6_77': 'acc', 'node_6_78': 'acc', 'node_6_79': 'acc', 'node_6_80': 'acc', 'node_6_81': 'acc', 'node_6_82': 'unacc', 'node_6_83': 'unacc', 'node_6_84': 'acc', 'node_6_85': 'acc', 'node_6_86': 'acc', 'node_6_87': 'acc', 'node_6_88': 'acc', 'node_6_89': 'unacc', 'node_6_90': 'good', 'node_6_91': 'acc', 'node_6_92': 'good', 'node_6_93': 'good', 'node_6_94': 'good', 'node_6_95': 'unacc', 'node_6_96': 'good', 'node_6_97': 'good', 'node_6_98': 'acc', 'node_6_99': 'acc', 'node_6_100': 'acc', 'node_6_101': 'unacc', 'node_6_102': 'acc', 'node_6_103': 'unacc', 'node_6_104': 'unacc', 'node_6_105': 'unacc', 'node_6_106': 'acc', 'node_6_107': 'acc', 'node_6_108': 'unacc', 'node_6_109': 'acc', 'node_6_110': 'acc', 'node_7_1': 'acc', 'node_7_2': 'unacc', 'node_7_3': 'acc', 'node_7_4': 'vgood', 'node_7_5': 'good', 'node_7_6': 'vgood', 'node_7_7': 'vgood', 'node_7_8': 'acc', 'node_7_9': 'acc', 'node_7_10': 'acc', 'node_7_11': 'unacc', 'node_7_12': 'good', 'node_7_13': 'unacc', 'node_7_14': 'good', 'node_7_15': 'acc', 'node_7_16': 'unacc', 'node_7_17': 'unacc', 'node_7_18': 'acc', 'node_7_19': 'acc', 'node_7_20': 'acc', 'node_7_21': 'unacc', 'node_7_22': 'acc', 'node_7_23': 'vgood', 'node_7_24': 'vgood', 'node_7_25': 'vgood', 'node_7_26': 'acc', 'node_7_27': 'unacc', 'node_7_28': 'acc', 'node_7_29': 'good', 'node_7_30': 'vgood', 'node_7_31': 'vgood', 'node_7_32': 'unacc', 'node_7_33': 'good', 'node_7_34': 'good', 'node_7_35': 'good', 'node_7_36': 'vgood', 'node_7_37': 'good', 'node_7_38': 'vgood', 'node_7_39': 'good', 'node_7_40': 'acc', 'node_7_41': 'vgood', 'node_7_42': 'vgood', 'node_7_43': 'vgood', 'node_7_44': 'good', 'node_7_45': 'good', 'node_7_46': 'vgood', 'node_7_47': 'vgood', 'node_7_48': 'acc', 'node_7_49': 'acc', 'node_7_50': 'vgood', 'node_7_51': 'good', 'node_7_52': 'vgood', 'node_7_53': 'unacc', 'node_7_54': 'acc', 'node_7_55': 'acc', 'node_7_56': 'acc', 'node_7_57': 'unacc', 'node_7_58': 'acc', 'node_7_59': 'unacc', 'node_7_60': 'acc', 'node_7_61': 'unacc', 'node_7_62': 'acc', 'node_7_63': 'acc', 'node_7_64': 'good', 'node_7_65': 'good', 'node_7_66': 'acc', 'node_7_67': 'acc', 'node_7_68': 'acc', 'node_7_69': 'good', 'node_7_70': 'acc', 'node_7_71': 'acc', 'node_7_72': 'unacc', 'node_7_73': 'acc', 'node_7_74': 'acc', 'node_7_75': 'unacc', 'node_7_76': 'acc', 'node_7_77': 'acc', 'node_7_78': 'acc', 'node_7_79': 'acc', 'node_7_80': 'good', 'node_7_81': 'good', 'node_7_82': 'unacc', 'node_7_83': 'acc', 'node_7_84': 'acc', 'node_7_85': 'unacc', 'node_7_86': 'unacc', 'node_7_87': 'acc', 'node_7_88': 'acc', 'node_7_89': 'acc', 'node_7_90': 'unacc', 'node_7_91': 'acc', 'node_7_92': 'acc', 'node_7_93': 'acc', 'node_7_94': 'unacc', 'node_7_95': 'acc', 'node_7_96': 'acc', 'node_7_97': 'acc', 'node_7_98': 'good', 'node_7_99': 'acc', 'node_7_100': 'good', 'node_7_101': 'unacc', 'node_7_102': 'acc', 'node_7_103': 'acc', 'node_7_104': 'acc', 'node_7_105': 'unacc', 'node_7_106': 'unacc', 'node_7_107': 'acc', 'node_7_108': 'acc', 'node_7_109': 'acc', 'node_7_110': 'unacc', 'node_7_111': 'acc', 'node_7_112': 'unacc', 'node_7_113': 'acc', 'node_7_114': 'acc', 'node_7_115': 'acc', 'node_7_116': 'unacc', 'node_7_117': 'acc', 'node_7_118': 'acc'}\n",
      "\n",
      " node types:\n",
      "{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal', 'node_3_1': 'Internal', 'node_3_2': 'Leaf', 'node_3_3': 'Internal', 'node_3_4': 'Internal', 'node_3_5': 'Internal', 'node_3_6': 'Leaf', 'node_4_1': 'Internal', 'node_4_2': 'Internal', 'node_4_3': 'Internal', 'node_4_4': 'Internal', 'node_4_5': 'Internal', 'node_4_6': 'Internal', 'node_4_7': 'Internal', 'node_4_8': 'Internal', 'node_4_9': 'Internal', 'node_4_10': 'Internal', 'node_4_11': 'Internal', 'node_4_12': 'Internal', 'node_4_13': 'Internal', 'node_4_14': 'Internal', 'node_4_15': 'Internal', 'node_4_16': 'Internal', 'node_5_1': 'Leaf', 'node_5_2': 'Leaf', 'node_5_3': 'Leaf', 'node_5_4': 'Internal', 'node_5_5': 'Internal', 'node_5_6': 'Internal', 'node_5_7': 'Internal', 'node_5_8': 'Internal', 'node_5_9': 'Leaf', 'node_5_10': 'Internal', 'node_5_11': 'Leaf', 'node_5_12': 'Leaf', 'node_5_13': 'Leaf', 'node_5_14': 'Internal', 'node_5_15': 'Internal', 'node_5_16': 'Internal', 'node_5_17': 'Leaf', 'node_5_18': 'Leaf', 'node_5_19': 'Leaf', 'node_5_20': 'Leaf', 'node_5_21': 'Internal', 'node_5_22': 'Internal', 'node_5_23': 'Internal', 'node_5_24': 'Leaf', 'node_5_25': 'Leaf', 'node_5_26': 'Leaf', 'node_5_27': 'Leaf', 'node_5_28': 'Leaf', 'node_5_29': 'Leaf', 'node_5_30': 'Internal', 'node_5_31': 'Internal', 'node_5_32': 'Leaf', 'node_5_33': 'Internal', 'node_5_34': 'Leaf', 'node_5_35': 'Internal', 'node_5_36': 'Leaf', 'node_5_37': 'Internal', 'node_5_38': 'Leaf', 'node_5_39': 'Internal', 'node_5_40': 'Internal', 'node_5_41': 'Leaf', 'node_5_42': 'Internal', 'node_5_43': 'Internal', 'node_5_44': 'Internal', 'node_5_45': 'Internal', 'node_5_46': 'Leaf', 'node_5_47': 'Internal', 'node_5_48': 'Internal', 'node_5_49': 'Leaf', 'node_5_50': 'Internal', 'node_5_51': 'Internal', 'node_5_52': 'Leaf', 'node_5_53': 'Internal', 'node_5_54': 'Internal', 'node_5_55': 'Internal', 'node_5_56': 'Internal', 'node_5_57': 'Leaf', 'node_5_58': 'Internal', 'node_5_59': 'Internal', 'node_5_60': 'Leaf', 'node_5_61': 'Leaf', 'node_5_62': 'Internal', 'node_6_1': 'Leaf', 'node_6_2': 'Internal', 'node_6_3': 'Leaf', 'node_6_4': 'Leaf', 'node_6_5': 'Leaf', 'node_6_6': 'Internal', 'node_6_7': 'Leaf', 'node_6_8': 'Internal', 'node_6_9': 'Leaf', 'node_6_10': 'Leaf', 'node_6_11': 'Internal', 'node_6_12': 'Leaf', 'node_6_13': 'Leaf', 'node_6_14': 'Leaf', 'node_6_15': 'Leaf', 'node_6_16': 'Internal', 'node_6_17': 'Leaf', 'node_6_18': 'Leaf', 'node_6_19': 'Internal', 'node_6_20': 'Leaf', 'node_6_21': 'Leaf', 'node_6_22': 'Internal', 'node_6_23': 'Internal', 'node_6_24': 'Leaf', 'node_6_25': 'Leaf', 'node_6_26': 'Leaf', 'node_6_27': 'Leaf', 'node_6_28': 'Internal', 'node_6_29': 'Internal', 'node_6_30': 'Leaf', 'node_6_31': 'Internal', 'node_6_32': 'Leaf', 'node_6_33': 'Leaf', 'node_6_34': 'Internal', 'node_6_35': 'Leaf', 'node_6_36': 'Leaf', 'node_6_37': 'Internal', 'node_6_38': 'Leaf', 'node_6_39': 'Leaf', 'node_6_40': 'Internal', 'node_6_41': 'Internal', 'node_6_42': 'Leaf', 'node_6_43': 'Leaf', 'node_6_44': 'Internal', 'node_6_45': 'Leaf', 'node_6_46': 'Leaf', 'node_6_47': 'Internal', 'node_6_48': 'Leaf', 'node_6_49': 'Leaf', 'node_6_50': 'Internal', 'node_6_51': 'Leaf', 'node_6_52': 'Leaf', 'node_6_53': 'Leaf', 'node_6_54': 'Internal', 'node_6_55': 'Leaf', 'node_6_56': 'Internal', 'node_6_57': 'Leaf', 'node_6_58': 'Leaf', 'node_6_59': 'Internal', 'node_6_60': 'Leaf', 'node_6_61': 'Leaf', 'node_6_62': 'Internal', 'node_6_63': 'Leaf', 'node_6_64': 'Leaf', 'node_6_65': 'Internal', 'node_6_66': 'Leaf', 'node_6_67': 'Leaf', 'node_6_68': 'Leaf', 'node_6_69': 'Leaf', 'node_6_70': 'Leaf', 'node_6_71': 'Internal', 'node_6_72': 'Leaf', 'node_6_73': 'Internal', 'node_6_74': 'Leaf', 'node_6_75': 'Leaf', 'node_6_76': 'Leaf', 'node_6_77': 'Leaf', 'node_6_78': 'Leaf', 'node_6_79': 'Internal', 'node_6_80': 'Internal', 'node_6_81': 'Leaf', 'node_6_82': 'Leaf', 'node_6_83': 'Leaf', 'node_6_84': 'Leaf', 'node_6_85': 'Leaf', 'node_6_86': 'Leaf', 'node_6_87': 'Internal', 'node_6_88': 'Leaf', 'node_6_89': 'Leaf', 'node_6_90': 'Leaf', 'node_6_91': 'Leaf', 'node_6_92': 'Leaf', 'node_6_93': 'Leaf', 'node_6_94': 'Internal', 'node_6_95': 'Internal', 'node_6_96': 'Leaf', 'node_6_97': 'Leaf', 'node_6_98': 'Internal', 'node_6_99': 'Leaf', 'node_6_100': 'Internal', 'node_6_101': 'Leaf', 'node_6_102': 'Leaf', 'node_6_103': 'Leaf', 'node_6_104': 'Internal', 'node_6_105': 'Leaf', 'node_6_106': 'Internal', 'node_6_107': 'Leaf', 'node_6_108': 'Leaf', 'node_6_109': 'Internal', 'node_6_110': 'Leaf', 'node_7_1': 'Leaf', 'node_7_2': 'Leaf', 'node_7_3': 'Leaf', 'node_7_4': 'Leaf', 'node_7_5': 'Leaf', 'node_7_6': 'Leaf', 'node_7_7': 'Leaf', 'node_7_8': 'Leaf', 'node_7_9': 'Leaf', 'node_7_10': 'Leaf', 'node_7_11': 'Leaf', 'node_7_12': 'Leaf', 'node_7_13': 'Leaf', 'node_7_14': 'Leaf', 'node_7_15': 'Leaf', 'node_7_16': 'Leaf', 'node_7_17': 'Leaf', 'node_7_18': 'Leaf', 'node_7_19': 'Leaf', 'node_7_20': 'Leaf', 'node_7_21': 'Leaf', 'node_7_22': 'Leaf', 'node_7_23': 'Leaf', 'node_7_24': 'Leaf', 'node_7_25': 'Leaf', 'node_7_26': 'Leaf', 'node_7_27': 'Leaf', 'node_7_28': 'Leaf', 'node_7_29': 'Leaf', 'node_7_30': 'Leaf', 'node_7_31': 'Leaf', 'node_7_32': 'Leaf', 'node_7_33': 'Leaf', 'node_7_34': 'Leaf', 'node_7_35': 'Leaf', 'node_7_36': 'Leaf', 'node_7_37': 'Leaf', 'node_7_38': 'Leaf', 'node_7_39': 'Leaf', 'node_7_40': 'Leaf', 'node_7_41': 'Leaf', 'node_7_42': 'Leaf', 'node_7_43': 'Leaf', 'node_7_44': 'Leaf', 'node_7_45': 'Leaf', 'node_7_46': 'Leaf', 'node_7_47': 'Leaf', 'node_7_48': 'Leaf', 'node_7_49': 'Leaf', 'node_7_50': 'Leaf', 'node_7_51': 'Leaf', 'node_7_52': 'Leaf', 'node_7_53': 'Leaf', 'node_7_54': 'Leaf', 'node_7_55': 'Leaf', 'node_7_56': 'Leaf', 'node_7_57': 'Leaf', 'node_7_58': 'Leaf', 'node_7_59': 'Leaf', 'node_7_60': 'Leaf', 'node_7_61': 'Leaf', 'node_7_62': 'Leaf', 'node_7_63': 'Leaf', 'node_7_64': 'Leaf', 'node_7_65': 'Leaf', 'node_7_66': 'Leaf', 'node_7_67': 'Leaf', 'node_7_68': 'Leaf', 'node_7_69': 'Leaf', 'node_7_70': 'Leaf', 'node_7_71': 'Leaf', 'node_7_72': 'Leaf', 'node_7_73': 'Leaf', 'node_7_74': 'Leaf', 'node_7_75': 'Leaf', 'node_7_76': 'Leaf', 'node_7_77': 'Leaf', 'node_7_78': 'Leaf', 'node_7_79': 'Leaf', 'node_7_80': 'Leaf', 'node_7_81': 'Leaf', 'node_7_82': 'Leaf', 'node_7_83': 'Leaf', 'node_7_84': 'Leaf', 'node_7_85': 'Leaf', 'node_7_86': 'Leaf', 'node_7_87': 'Leaf', 'node_7_88': 'Leaf', 'node_7_89': 'Leaf', 'node_7_90': 'Leaf', 'node_7_91': 'Leaf', 'node_7_92': 'Leaf', 'node_7_93': 'Leaf', 'node_7_94': 'Leaf', 'node_7_95': 'Leaf', 'node_7_96': 'Leaf', 'node_7_97': 'Leaf', 'node_7_98': 'Leaf', 'node_7_99': 'Leaf', 'node_7_100': 'Leaf', 'node_7_101': 'Leaf', 'node_7_102': 'Leaf', 'node_7_103': 'Leaf', 'node_7_104': 'Leaf', 'node_7_105': 'Leaf', 'node_7_106': 'Leaf', 'node_7_107': 'Leaf', 'node_7_108': 'Leaf', 'node_7_109': 'Leaf', 'node_7_110': 'Leaf', 'node_7_111': 'Leaf', 'node_7_112': 'Leaf', 'node_7_113': 'Leaf', 'node_7_114': 'Leaf', 'node_7_115': 'Leaf', 'node_7_116': 'Leaf', 'node_7_117': 'Leaf', 'node_7_118': 'Leaf'}\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation on training dataframe:\n",
    "tree_model = tree_train(train_df, 0.9)\n",
    "[tree_connectivity, node_labels, node_types] = tree_model\n",
    "\n",
    "print(\"\\n tree connectivity:\")\n",
    "print(tree_connectivity)\n",
    "\n",
    "print(\"\\n node labels:\")\n",
    "print(node_labels)\n",
    "\n",
    "print(\"\\n node types:\")\n",
    "print(node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty1X5YsJ25UT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Part 7: Prediction by the Desicion Tree\n",
    "(Q.7., **20 Marks**): Following the completion of decision tree training, the next step is to implement the prediction process through the trained tree structure. To achieve this, we need to create a function named 'tree_prediction.' This function takes two inputs: a test dataframe containing the samples to be predicted and the trained decision tree. It returns the predicted labels generated by the decision tree as a single DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0Yi1bTnb3rEE"
   },
   "outputs": [],
   "source": [
    "def tree_prediction(testing_data, tree_model):\n",
    "\n",
    "    pred_labels = []\n",
    "    # Unpack the tree_model list into three separate variables: tree_connectivity, node_labels, and node_types\n",
    "    [tree_connectivity, node_labels, node_types] = tree_model\n",
    "    \n",
    "    # Iterate through each sample in the testing_data\n",
    "    for i in range(len(testing_data)):\n",
    "        # Get a sample from the testing dataset\n",
    "        sample = testing_data.loc[i]\n",
    "        \n",
    "        # Start at the root node, which is always named \"node_1_1\"\n",
    "        current_node = \"node_1_1\"\n",
    "        \n",
    "        #.................................\n",
    "        # write the rest here:\n",
    "        # Begin a loop to traverse the decision tree until a leaf node is reached\n",
    "        current_type = node_types[current_node]\n",
    "\n",
    "        while current_type != \"Leaf\": #keep going until u reach a leaf\n",
    "            split_atr = tree_connectivity[current_node][\"split_attribute\"]\n",
    "            value = sample[split_atr]\n",
    "            \n",
    "            if str(value) not in tree_connectivity[current_node][\"attribute_value\"]: #if the value doesnt exist, return the parents label\n",
    "                break\n",
    "            \n",
    "            current_node = tree_connectivity[current_node][\"attribute_value\"][str(value)]\n",
    "            current_type = node_types[current_node]\n",
    "            \n",
    "        pred_labels.append(node_labels[current_node])\n",
    "    \n",
    "    # find the node label and put it in the pred_labels Pandas Series\n",
    "    #.................................\n",
    "    # Return the Pandas Series containing the predicted labels\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gtIABZziDgSR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'acc', 'good', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'vgood', 'vgood', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'acc', 'acc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'vgood', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'vgood', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'good', 'unacc', 'unacc', 'vgood', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'good', 'unacc', 'unacc', 'good', 'unacc', 'vgood', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'vgood', 'unacc', 'good', 'unacc', 'unacc', 'acc', 'good', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'good', 'acc', 'acc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'good', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'good', 'vgood', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'vgood', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'acc', 'unacc', 'acc', 'unacc', 'good', 'vgood', 'acc', 'acc', 'acc', 'good', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'acc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc', 'unacc']\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation on training dataframe:\n",
    "tree_model = tree_train(train_df, 0.9)\n",
    "pred_labels = tree_prediction(train_df, tree_model)\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRVHjnEi5lvn"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: Evaluating the Model\n",
    "\n",
    "* (Q.8-a, **5 Marks**)In the final step of this assignment, you'll apply the decision tree learning process. Start by training the decision tree on the training dataset using the 'tree_train' function, setting the terminating threshold to 0.9. Next, employ the 'tree_prediction' function, as previously implemented, to generate predictions for both the training and testing datasets. Following this, your task is to compare these predicted labels with the actual ground-truth labels to compute and report the accuracy rates for both the training and testing datasets.\n",
    "\n",
    "* (Q.8-b, **5 Marks**) Now, repeat the process with a different terminating threshold, specifically 0.7, and once again calculate and report the accuracy rates for the training and testing datasets. Finally, compare and contrast the results obtained with the two different threshold values (0.9 and 0.7). Provide an analysis and discussion of why one threshold might yield higher accuracy compared to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "D9Jt2EBp6uuU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for threshold:  0.9\n",
      "1398 out of 1400 accuracy =  0.9985714285714286\n",
      "302 out of 327 accuracy =  0.9235474006116208\n",
      "\n",
      "for threshold:  0.7\n",
      "997 out of 1400 accuracy =  0.7121428571428572\n",
      "213 out of 327 accuracy =  0.6513761467889908\n",
      "\n",
      "for threshold:  0.8\n",
      "1394 out of 1400 accuracy =  0.9957142857142857\n",
      "304 out of 327 accuracy =  0.9296636085626911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#.................................\n",
    "# write the rest here:\n",
    "def compare_accurcy(train_df, test_df, thresh):\n",
    "    print(\"for threshold: \", thresh)\n",
    "    tree_model = tree_train(train_df, thresh)\n",
    "    train_pred_labels = tree_prediction(train_df, tree_model)\n",
    "    test_pred_labels = tree_prediction(test_df, tree_model)\n",
    "    \n",
    "    train_correct = (train_pred_labels == train_df[train_df.columns[-1]]).sum()\n",
    "    siz = len(train_df.index)\n",
    "    print (train_correct, \"out of\", siz, \"accuracy = \", train_correct/siz)\n",
    "    \n",
    "    test_correct = (test_pred_labels == test_df[train_df.columns[-1]]).sum()\n",
    "    siz = len(test_df.index)\n",
    "    print (test_correct, \"out of\", siz, \"accuracy = \", test_correct/siz)\n",
    "    print()\n",
    "    \n",
    "compare_accurcy(train_df, test_df, 0.9)\n",
    "compare_accurcy(train_df, test_df, 0.7)\n",
    "compare_accurcy(train_df, test_df, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the threshold is lower, the tree is going to stop splitting sooner\n",
    "\n",
    "in case of 0.7: \n",
    "the leaves arent pure enough. it barely matches the training set, so its expected that testing accuracy is low. we need to split the nodes furthur.\n",
    "the model is underfitted. both training and testing accuracy are low (keep in mind that 1000/1400 labels are UNACC. so if u just return UNAC ull get the same accuracy of 0.71). \n",
    "\n",
    "in case of 0.9: \n",
    "since 0.8 gives us a better test accuracy and lower train accuracy,\n",
    "the model is (a tiny,tiny,tiny bit) overfitted.\n",
    "\n",
    "in case of 0.8:\n",
    "the model performs well both on training and testing.\n",
    "the model is good.\n",
    "\n",
    "(since the data is skewed, maybe accuracy isnt the best messurement) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "unacc    997\n",
       "acc      298\n",
       "good      55\n",
       "vgood     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMseztk+f92ev7YF44ZdQiz",
   "provenance": [
    {
     "file_id": "19CmDcPf84La1mTvE0Gi0VE-ILrGARQt8",
     "timestamp": 1695758816173
    },
    {
     "file_id": "1_0_f28q3RsHEbCI9TXMfcytOE-A_8xU7",
     "timestamp": 1695591943950
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
